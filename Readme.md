## Packages Used
1. scikit-learn 0.17.1 http://scikit-learn.org/stable/
   <br> sudo pip install scikit-learn
2. pandas 0.18.1 http://pandas.pydata.org/ <br> sudo pip install pandas
3. numpy 1.11.1 http://www.numpy.org/ <br> sudo pip install numpy
4. xgboost 0.6 https://github.com/dmlc/xgboost/tree/master/python-package 
5. keras 1.0.8 https://keras.io/#installation <br> sudo pip install keras

## Dependencies for packages 
1. scipy 0.18.0 https://www.scipy.org/ <br> sudo pip install scipy
2. Theano 0.8.2 http://deeplearning.net/software/theano/install.html#install <br> sudo pip install Theano
3. pyyaml 3.11 https://pypi.python.org/pypi/PyYAML/3.12 <br> sudo pip install pyyaml
4. gcc 4.8.4 <br> sudo apt-get install build-essential

## Procedure to generate final output file:
1. Keep all the training and testing files and the scripts in one folder and set the working directory to that folder. 
2. Run gbm.py to generate intermediate predictions with gbm model. (Train and Test file locations specified in lines 1,2)
3. Run xgb.py to generate intermediate predictions with xgb model. (Train and Test file locations specified in lines 1,2)
4. Run keras_try.py to generate intermediate predictions with keras neural network. (Train and Test file locations specified in lines 1,2)
5. Run averager.py to generate the final predictions with an average for all models. (Uses intermediate files generated by previous scripts and generates final prediction.csv) (Location of intermediate files specified in lines 3,4,5)

## Approach
I started with filling the missing values with the pad method ( i.e copying from the previous row ) since the attribute of a park should be similar to what it was the last day. A better approach could have been, to do this individually for each park. Then for the feature selection, I observed that the month of the year and the day of the month were highly influential in determining the footfall.

I made a 0/1 feature for winters ( months 11,12,1,2,3 ). For the dates, I observed some pattern, in the variation of mean footfall with increasing dates. But there were some anomalies which I tried to treat by averaging across adjacent days. (I did this for all the parks together, a better approach could have been to do this for each park). I also binned the direction of wind to represent the 4 directions.

For the modeling part, I started with gradient boosted trees and further tuned it to get the best result in cv (by testing on years 2000-2001) and then I tried xgboost and tuned it. Finally, I made a 1 hidden layer neural network with a wide hidden layer. I averaged the results of these 3 models to get the final output.

In addition to all that, for the gbm and xgboost models,I trained the regressors for each park independently as I believed that each park will have an independent pattern and relationship with other variables. For the neural net model, I trained it for all parks together and giving in park id as a feature as it needed larger number of samples to be trained.
